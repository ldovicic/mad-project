Author
dovicic2

================================================================

Submitted files and folders:
- 'project' folder
	- notable files:
		- spiders/cars_spider.py
		- spiders/car_details_spider.py
	- the rest is a byproduct of using scrapy (configuration files, etc.)

- 'web' folder
	- contains the React project

- car_links.txt
- car_links_unique.txt

- database.db

- protocol.txt

- scrapy.cfg

- unique.py

- report.pdf - web version is recommended instead, pdf is not formatted too well

================================================================

PART I + II - Acquiring and preprocessing data

cars_spider.py
- has to be run from the main folder
command to run: `scrapy crawl cars`

unique.py
- has to be run from the main folder as a regular Python script

car_details_spider.py
- has to be run from the main folder
command to run: `scrapy crawl cardetails`

After the scripts finish running, manually validating some data using SQL queries:

select distinct make from car;
Verify there are no duplicate makes (e.g. VW and Volkswagen) and filter out wrongly selected data (e.g. '1975' instead of the actual make)

select * from car where transmission != 'manual' and transmission != 'automatic';
Search for different entries that weren't caught by our parser. Manually (update ...) correct the values.  E.g. 'single speed gear box' -> 'automatic'.

Similarly
select * from car where drivetrain != 'AWD' and drivetrain != 'FWD' and drivetrain != 'RWD';
Correct or remove these values.

The database file had to be copied into the web folder.

PART III

Go to the web folder and run
npm install
(requires node and npm)
then to start the server run
npm run dev

================================================================
Sources:
relevant documentation for technologies used1
autobazar.eu (data)
Stack Exchange
Claude Opus (AI)
Figma